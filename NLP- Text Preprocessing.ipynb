{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bdf40e9-909d-4e68-8760-673830374b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0519e19-eb0f-43e4-a084-bccd4e26d4a5",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fc01aa-64e9-4476-b050-9c5ea8914c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello i am currently learning generative ai,\n",
      "Now i am learning nlp.\n",
      "After that i will learn generative ai.\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello i am currently learning generative ai,\n",
    "Now i am learning nlp.\n",
    "After that i will learn generative ai.\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9cc13f0-42a4-40c0-a431-d565ed125f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello i am currently learning generative ai,\n",
      "Now i am learning nlp.\n",
      "After that i will learn generative ai.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "documents=sent_tokenize(corpus)\n",
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b009c96-c028-4959-9d44-2abaa17399fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'generative',\n",
       " 'ai',\n",
       " ',',\n",
       " 'Now',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'After',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'generative',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413e61f7-d26f-412e-b3b3-a5c2dab26cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'i', 'am', 'currently', 'learning', 'generative', 'ai', ',', 'Now', 'i', 'am', 'learning', 'nlp', '.']\n",
      "['After', 'that', 'i', 'will', 'learn', 'generative', 'ai', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd6527a-19d2-4f65-b563-b71ad5866316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'generative',\n",
       " 'ai',\n",
       " ',',\n",
       " 'Now',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'After',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'generative',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf75bfdb-6e64-42c5-af8f-8beb426ab7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'currently',\n",
       " 'learning',\n",
       " 'generative',\n",
       " 'ai',\n",
       " ',',\n",
       " 'Now',\n",
       " 'i',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'nlp.',\n",
       " 'After',\n",
       " 'that',\n",
       " 'i',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'generative',\n",
       " 'ai',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96517e79-3ca2-4442-b37b-e2265ca431da",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "05b2953c-2079-4c66-9ccc-b2e722a9d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"finally\",\"history\",\"finalized\",\"programs\",\"programming\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ce2f7-ae58-496e-b540-45e8c4a6bc35",
   "metadata": {},
   "source": [
    "### PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1526497b-dc2c-4b61-a3a5-211da1020099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8190cc28-0b11-4e1c-aa4c-5851473ba6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "finally---->final\n",
      "history---->histori\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+ \"---->\"+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2928ecd2-4d92-4394-9cd4-0a16137a7345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sit'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem(\"sitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b42c3bc-ad85-4998-9650-4af863969a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'),stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b64fe9-67b2-4890-9478-11aad43e1ab4",
   "metadata": {},
   "source": [
    "### RegexpStemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d5c67e5-61bc-4f02-9314-fc11927a15c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fea3ca4-ffa0-4fbf-83d7-727880f29ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer('ing$|s$|able$|en$',min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dca9396-695d-4282-8d48-8808de66dd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "11eca07a-458c-48d6-8e58-3bddf13080d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem('eaten')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31e185-ec1d-4468-bf68-5d93fe389a19",
   "metadata": {},
   "source": [
    "### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82b88ac5-8839-4ec5-bd65-e91399662c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35957261-9462-47c1-8bdc-0b19c9631054",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9be111d-6cc9-49f2-83a8-839922bb5fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eaten\n",
      "writing---->write\n",
      "writes---->write\n",
      "finally---->final\n",
      "history---->histori\n",
      "finalized---->final\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+snowballstemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "be84533d-07b5-4b98-a217-a946b0311f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowballstemmer.stem('fairly'),snowballstemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2246188-1957-4055-8056-6fe0522cb540",
   "metadata": {},
   "source": [
    " ## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a64e41ee-acda-4e18-93c4-2a20695e512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\USER/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fc39f94f-d4e3-401e-a511-ea4b922b5d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2d83b66-2014-4d70-a2a9-657f2d3b874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36d3677c-4aca-4dc2-97f6-afa9c6ad05c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "POS -\n",
    "Noun-n\n",
    "verb-v\n",
    "adjective-a\n",
    "adverb-r\n",
    "'''\n",
    "lemmatizer.lemmatize(\"going\",pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "50688334-458e-4cba-8272-d370c8d59515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eats---->eat\n",
      "eaten---->eat\n",
      "writing---->write\n",
      "writes---->write\n",
      "finally---->finally\n",
      "history---->history\n",
      "finalized---->finalize\n",
      "programs---->program\n",
      "programming---->program\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bc7d2ae9-64b8-43ca-b71e-c5b82ea34f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairly', 'sportingly')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('fairly',pos='n'), lemmatizer.lemmatize('sportingly',pos='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04929210-c105-4d47-8933-4434fc7403b4",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b8df51e6-690c-4575-b66a-6b4a34cf377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "61634a72-aca3-4ea9-b8d9-1b82e2add4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "571872ef-e091-470c-970e-9251800e516c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4ff8dfa0-7579-49a5-960a-95c5bcf0c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Text preprocessing is set of techniques used to clean and prepare raw text so that it can be understood by machine learning or \n",
    "deep learning models. Raw text is nothing but spelling mistakes,symbols,stopwords. Machine learning doesn't understand text directly they understand\n",
    "numbers or vectors. Preprocessing converts raw text to structured , clean,meaningful vectors. Text preprocessing is important because it remove\n",
    "noise(symbols,punctuations), it reduce vocablury size ,improve accuracy and speeds up the traning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "50039668-d3cb-489f-8e5a-ceb6e8718dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c55369f1-b90b-41ef-b4f5-fbacded18a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9a7d14da-391f-4c06-8b24-8cc54a8bee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4500d13c-f4fe-4526-b931-91d03ae6283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get english stopwords\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "db28c28d-64e3-4486-8845-6b62ba5083d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "filtered_words=[word for word in words if word.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d3565e26-17bb-4ed6-8ce6-118870b31178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WORDS: ['Text', 'preprocessing', 'is', 'set', 'of', 'techniques', 'used', 'to', 'clean', 'and', 'prepare', 'raw', 'text', 'so', 'that', 'it', 'can', 'be', 'understood', 'by', 'machine', 'learning', 'or', 'deep', 'learning', 'models', '.', 'Raw', 'text', 'is', 'nothing', 'but', 'spelling', 'mistakes', ',', 'symbols', ',', 'stopwords', '.', 'Machine', 'learning', 'does', \"n't\", 'understand', 'text', 'directly', 'they', 'understand', 'numbers', 'or', 'vectors', '.', 'Preprocessing', 'converts', 'raw', 'text', 'to', 'structured', ',', 'clean', ',', 'meaningful', 'vectors', '.', 'Text', 'preprocessing', 'is', 'important', 'because', 'it', 'remove', 'noise', '(', 'symbols', ',', 'punctuations', ')', ',', 'it', 'reduce', 'vocablury', 'size', ',', 'improve', 'accuracy', 'and', 'speeds', 'up', 'the', 'traning', '.']\n",
      "FILTERED WORDS: ['Text', 'preprocessing', 'set', 'techniques', 'used', 'clean', 'prepare', 'raw', 'text', 'understood', 'machine', 'learning', 'deep', 'learning', 'models', '.', 'Raw', 'text', 'nothing', 'spelling', 'mistakes', ',', 'symbols', ',', 'stopwords', '.', 'Machine', 'learning', \"n't\", 'understand', 'text', 'directly', 'understand', 'numbers', 'vectors', '.', 'Preprocessing', 'converts', 'raw', 'text', 'structured', ',', 'clean', ',', 'meaningful', 'vectors', '.', 'Text', 'preprocessing', 'important', 'remove', 'noise', '(', 'symbols', ',', 'punctuations', ')', ',', 'reduce', 'vocablury', 'size', ',', 'improve', 'accuracy', 'speeds', 'traning', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WORDS:\",words)\n",
    "print(\"FILTERED WORDS:\",filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2507fcf9-9217-4357-aeba-a2bffbb51f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "124e47da-94ee-4cad-ae56-90438195cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d718d99c-9f03-4a99-b61b-463fb099e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d21c7712-0391-4f12-92e5-e3c7acf79537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5ee690ef-64c7-4343-af4b-115655f42c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply stopwords and filter and then apply Porter stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) #converting all the list of words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d2be41e6-87b2-4d8c-ad92-6d32b6fdf1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text preprocess set techniqu use clean prepar raw text understood machin learn deep learn model .', 'raw text noth spell mistak , symbol , stopword .', \"machin learn n't understand text directli understand number vector .\", 'preprocess convert raw text structur , clean , meaning vector .', 'text preprocess import remov nois ( symbol , punctuat ) , reduc vocabluri size , improv accuraci speed trane .']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c8377a0d-c4ae-4cea-8ea2-28122adafd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2354ad36-290f-48c4-a14e-b4aa1ce50443",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowballstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ab89b789-1302-400b-9ed5-ee537ee369be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "be820c0b-f141-4863-9630-d7cfc33d2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply stopword and filter then apply snowball stemming\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b5958715-2f6b-49e0-a320-485cb0fd037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text preprocess set techniqu use clean prepar raw text understood machin learn deep learn model .', 'raw text noth spell mistak , symbol , stopword .', \"machin learn n't understand text direct understand number vector .\", 'preprocess convert raw text structur , clean , meaning vector .', 'text preprocess import remov nois ( symbol , punctuat ) , reduc vocabluri size , improv accuraci speed trane .']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc883d8b-f857-4b72-975e-43e450f02042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8d1f74a8-0922-4bbf-89a2-d8f4cbaeb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e32a7da9-4617-42a5-be40-1183e03df465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "bc26c87f-c0e7-4756-97e8-17c12311c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply stopwords and filter and then apply lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "58c65905-09b8-48ad-9896-df88a8a44b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text preprocessing set technique use clean prepare raw text understand machine learn deep learn model .', 'raw text nothing spell mistake , symbol , stopwords .', \"machine learn n't understand text directly understand number vector .\", 'preprocessing convert raw text structure , clean , meaningful vector .', 'text preprocessing important remove noise ( symbol , punctuation ) , reduce vocablury size , improve accuracy speed traning .']\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee4bee-e728-4f2d-8c4b-ade0204675dc",
   "metadata": {},
   "source": [
    " ## Parts of speech (POS) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d90b56db-a6f9-449b-932f-17a6c300dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "99005df5-5a9d-4d31-ae58-6c6c6157eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER/nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6b1112cd-f1b5-4316-8508-53f0831ed182",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='NLP is field of artificial intelligence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "41d2fe32-a53a-4746-ab05-13e16ca09dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9158dae3-1ba3-4d4d-b222-1c69d6770b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8bd9fad4-b132-445b-93fd-0b13cf5c39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos tagging\n",
    "pos_tags=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "31d504e6-d3b8-4bc3-92c6-73c3f4ac8621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f3e97-60ff-4ff2-a05a-32698e1303d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
