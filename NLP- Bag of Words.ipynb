{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f108b6-1593-41eb-9d57-882f5644f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "messages=pd.read_csv('Documents/spam_ham.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab8ef52-9ac8-49f1-8f33-674001971487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>I'm gonna be home soon and i don't want to tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spam</td>\n",
       "      <td>URGENT! You have won a 1 week FREE membership ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ham</td>\n",
       "      <td>I’ve been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  WINNER!! As a valued network customer you have...\n",
       "6   ham  I'm gonna be home soon and i don't want to tal...\n",
       "7  spam  URGENT! You have won a 1 week FREE membership ...\n",
       "8   ham  I’ve been searching for the right words to tha..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49fde25c-e184-4365-a2f6-6912fcf1ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning and preprocessing\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895dc721-23dd-496f-aeb8-9d2825d4ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92143d92-758b-4883-ac7f-6dd587fce26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "921f22cd-8c6f-4f95-a9b6-2b83b2babc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go jurong point crazi avail bugi n great world la e buffet', 'ok lar joke wif u oni', 'free entri wkli comp win fa cup final tkt st may', 'u dun say earli hor u c alreadi say', 'nah think goe usf live around though', 'winner valu network custom select receiv prize reward', 'gonna home soon want talk stuff anymor tonight', 'urgent week free membership prize jackpot', 'search right word thank breather']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9dd33f90-8541-4a6a-ad72-922221724288",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for binary bag of words enable binary=True\n",
    "cv=CountVectorizer(max_features=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddd2cc3c-8867-4b25-a8d5-d0e6d571222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "117746a6-ff92-42db-a86f-570c1e3a6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus=[\"I love NLP\" ,\"NLP loves python\"]\n",
    "vectorizer=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5730c291-ab29-4ae3-b073-c7dd92c9c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8819f8ce-3ac6-4436-8d3b-9b2ff6ad10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['love' 'loves' 'nlp' 'python']\n",
      "BoW vector:\n",
      " [[1 0 1 0]\n",
      " [0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW vector:\\n\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2e35c4-6201-4042-84d4-49ebfb6a1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-Grams\n",
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35f263b8-1d4b-4612-92f9-e1d4fefcb177",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"I Love NLP\"\n",
    "tokens=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "724a731c-dc39-4374-815f-6174674fe9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram=list(ngrams(tokens,1))\n",
    "bigram=list(ngrams(tokens,2))\n",
    "trigram=list(ngrams(tokens,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8395bb4-ad73-4a67-b034-7c254f5b3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ['I', 'Love', 'NLP']\n",
      "Unigram [('I',), ('Love',), ('NLP',)]\n",
      "Bigram [('I', 'Love'), ('Love', 'NLP')]\n",
      "Trigram [('I', 'Love', 'NLP')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens\",tokens)\n",
    "print(\"Unigram\",unigram)\n",
    "print(\"Bigram\",bigram)\n",
    "print(\"Trigram\",trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f530954-de04-4c59-b391-ad2d44cf6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using sklearn countvectorizer with ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09f1cfee-56d6-4b12-8039-1ebba0b51737",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[\"I love nlp\",\n",
    "       \"Nlp loves python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c01b67b-6e7b-4157-9fe5-90e35b1cd8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary ['love' 'love nlp' 'loves' 'loves python' 'nlp' 'nlp loves' 'python']\n",
      "Vectors: [[1 1 0 0 1 0 0]\n",
      " [0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer(ngram_range=(1,2))\n",
    "x=vectorizer.fit_transform(corpus)\n",
    "print(\"vocabulary\",vectorizer.get_feature_names_out())\n",
    "print(\"Vectors:\",x.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4d577-bb89-45ab-955c-ac5edd64e7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f35086a-e40d-4294-89c7-20479b4adb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
